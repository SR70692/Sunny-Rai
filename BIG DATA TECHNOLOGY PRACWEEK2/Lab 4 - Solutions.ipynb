{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries (numpy, pandas, scikit-learn packages metrics to start)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wine dataset available at https://archive.ics.uci.edu/ml/datasets/Wine\n",
    "wine = pd.read_csv(\"wine.data\", header=None) # in the folder datasets located in the same folder as the python .ipynb\n",
    "# header = none is used as the dataset doesn't have a header in the wine.data\n",
    "# of course more information about the columns are available online at https://archive.ics.uci.edu/ml/datasets/Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1     2     3     4    5     6     7     8     9      10    11  \\\n",
       "0     1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29   5.64  1.04   \n",
       "1     1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28   4.38  1.05   \n",
       "2     1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81   5.68  1.03   \n",
       "3     1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18   7.80  0.86   \n",
       "4     1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82   4.32  1.04   \n",
       "..   ..    ...   ...   ...   ...  ...   ...   ...   ...   ...    ...   ...   \n",
       "173   3  13.71  5.65  2.45  20.5   95  1.68  0.61  0.52  1.06   7.70  0.64   \n",
       "174   3  13.40  3.91  2.48  23.0  102  1.80  0.75  0.43  1.41   7.30  0.70   \n",
       "175   3  13.27  4.28  2.26  20.0  120  1.59  0.69  0.43  1.35  10.20  0.59   \n",
       "176   3  13.17  2.59  2.37  20.0  120  1.65  0.68  0.53  1.46   9.30  0.60   \n",
       "177   3  14.13  4.10  2.74  24.5   96  2.05  0.76  0.56  1.35   9.20  0.61   \n",
       "\n",
       "       12    13  \n",
       "0    3.92  1065  \n",
       "1    3.40  1050  \n",
       "2    3.17  1185  \n",
       "3    3.45  1480  \n",
       "4    2.93   735  \n",
       "..    ...   ...  \n",
       "173  1.74   740  \n",
       "174  1.56   750  \n",
       "175  1.56   835  \n",
       "176  1.62   840  \n",
       "177  1.60   560  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wine vs. wine.values\n",
    "wine\n",
    "#wine.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an exploratory data analysis (EDA) on the dataset to better understand its underlying structure and key features. \n",
    "# Typical steps in EDA include examining the data’s shape and structure, checking for missing values, \n",
    "# visualizing distributions, and identifying relationships between variables through basic statistical summaries and \n",
    "# visualizations like histograms, box plots, and scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the class of wine (first column) and remaining data (attributes) so we can use the attributes in our classifiers\n",
    "# Segment the outcome (first column) and remaining data (attributes) so we can use the attributes for clustering\n",
    "wineData = wine.values[:, 1:14] # remember that the last column 14 is actually excluded so only 1 to 13 will be selected\n",
    "wineTarget = wine.values[:,0] # we know that the true target is in the first column for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "# once segmented, now we can print the data to check it\n",
    "print(wineData)\n",
    "\n",
    "# we can also print the target or the true values if needed\n",
    "print(wineTarget)\n",
    "\n",
    "# we can also print the unique values of the target\n",
    "# print(np.unique(wineTarget))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into two subsets: training and test set\n",
    "from sklearn import model_selection\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(wineData, wineTarget, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.296e+01 3.450e+00 2.350e+00 ... 6.800e-01 1.750e+00 6.750e+02]\n",
      " [1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.237e+01 1.170e+00 1.920e+00 ... 1.120e+00 3.480e+00 5.100e+02]\n",
      " ...\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " [1.277e+01 2.390e+00 2.280e+00 ... 5.700e-01 1.630e+00 4.700e+02]\n",
      " [1.305e+01 1.770e+00 2.100e+00 ... 8.800e-01 3.350e+00 8.850e+02]]\n",
      "[3. 1. 2. 1. 3. 3. 1. 3. 3. 3. 1. 1. 1. 2. 2. 1. 1. 1. 1. 3. 2. 2. 1. 1.\n",
      " 3. 1. 2. 2. 1. 1. 3. 3. 2. 2. 2. 3. 3. 2. 1. 2. 2. 1. 1. 2. 1. 1. 1. 3.\n",
      " 3. 3. 2. 2. 1. 2. 1. 3. 2. 1. 1. 2. 3. 3. 1. 2. 1. 2. 2. 2. 2. 2. 1. 2.\n",
      " 3. 1. 3. 1. 2. 1. 3. 2. 1. 1. 2. 2. 1. 2. 3. 2. 1. 2. 2. 3. 2. 2. 1. 2.\n",
      " 2. 2. 2. 3. 3. 2. 3. 2. 3. 2. 1. 3. 2. 3. 2. 1. 2. 2. 2. 3. 1. 2. 1. 1.\n",
      " 2. 1. 3. 1.]\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, Y_train, Y_test\n",
    "print(X_train)\n",
    "#print(X_test)\n",
    "print(Y_train)\n",
    "#print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "**************************************\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m      4\u001b[0m lm \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m----> 5\u001b[0m lm\u001b[38;5;241m.\u001b[39mfit(X_train, Y_train)\n\u001b[0;32m      6\u001b[0m predicted \u001b[38;5;241m=\u001b[39m lm\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mclassification_report(Y_test, predicted))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the following classifiers to predict the class of wine\n",
    "# i. Logistic regression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances\n",
    "# recall (also known as sensitivity) is the fraction of the total amount of relevant instances that were actually retrieved\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"**************************************\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lm = LogisticRegression()\n",
    "lm.fit(X_train, Y_train)\n",
    "predicted = lm.predict(X_test)\n",
    "print(metrics.classification_report(Y_test, predicted))\n",
    "print(metrics.confusion_matrix(Y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " K nearest neighbours\n",
      "**************************************\n",
      "KNeighborsClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.88      0.88      0.88        16\n",
      "         2.0       0.57      0.76      0.65        21\n",
      "         3.0       0.60      0.35      0.44        17\n",
      "\n",
      "    accuracy                           0.67        54\n",
      "   macro avg       0.68      0.66      0.66        54\n",
      "weighted avg       0.67      0.67      0.65        54\n",
      "\n",
      "[[14  1  1]\n",
      " [ 2 16  3]\n",
      " [ 0 11  6]]\n"
     ]
    }
   ],
   "source": [
    "# Create the following classifiers to predict the class of wine\n",
    "# K nearest neighbours\n",
    "print(\"\\n\\n K nearest neighbours\")\n",
    "print(\"**************************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "print(model)\n",
    "predicted = model.predict(X_test)\n",
    "print(metrics.classification_report(Y_test, predicted))\n",
    "print(metrics.confusion_matrix(Y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " K nearest neighbours - weights: distance\n",
      "**************************************\n",
      "KNeighborsClassifier(weights='distance')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.88      0.88      0.88        16\n",
      "         2.0       0.60      0.71      0.65        21\n",
      "         3.0       0.62      0.47      0.53        17\n",
      "\n",
      "    accuracy                           0.69        54\n",
      "   macro avg       0.70      0.69      0.69        54\n",
      "weighted avg       0.69      0.69      0.68        54\n",
      "\n",
      "[[14  1  1]\n",
      " [ 2 15  4]\n",
      " [ 0  9  8]]\n"
     ]
    }
   ],
   "source": [
    "# Create the following classifiers to predict the class of wine\n",
    "# K nearest neighbours, with different parameters if needed\n",
    "print(\"\\n\\n K nearest neighbours - weights: distance\")\n",
    "print(\"**************************************\")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(weights='distance')\n",
    "model.fit(X_train, Y_train)\n",
    "print(model)\n",
    "predicted = model.predict(X_test)\n",
    "print(metrics.classification_report(Y_test, predicted))\n",
    "print(metrics.confusion_matrix(Y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree\n",
      "**************************************\n",
      "DecisionTreeClassifier()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      0.81      0.90        16\n",
      "         2.0       0.78      1.00      0.88        21\n",
      "         3.0       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.89        54\n",
      "   macro avg       0.93      0.88      0.89        54\n",
      "weighted avg       0.91      0.89      0.89        54\n",
      "\n",
      "[[13  3  0]\n",
      " [ 0 21  0]\n",
      " [ 0  3 14]]\n"
     ]
    }
   ],
   "source": [
    "# Create the following classifiers to predict the class of wine\n",
    "# Decision tree\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "print(\"\\nDecision Tree\")\n",
    "print(\"**************************************\")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "#model = DecisionTreeClassifier(criterion='entropy')\n",
    "model.fit(X_train, Y_train)\n",
    "print(model)\n",
    "predicted = model.predict(X_test)\n",
    "print(metrics.classification_report(Y_test, predicted))\n",
    "print(metrics.confusion_matrix(Y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes\n",
      "**************************************\n",
      "GaussianNB()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      0.94      0.97        16\n",
      "         2.0       0.95      0.90      0.93        21\n",
      "         3.0       0.89      1.00      0.94        17\n",
      "\n",
      "    accuracy                           0.94        54\n",
      "   macro avg       0.95      0.95      0.95        54\n",
      "weighted avg       0.95      0.94      0.94        54\n",
      "\n",
      "[[15  1  0]\n",
      " [ 0 19  2]\n",
      " [ 0  0 17]]\n"
     ]
    }
   ],
   "source": [
    "# 5. Create the following classifiers to predict the class of wine\n",
    "# iv. Naïve Bayes\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "print(\"\\nNaive Bayes\")\n",
    "print(\"**************************************\")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, Y_train)\n",
    "print(model)\n",
    "predicted = model.predict(X_test)\n",
    "print(metrics.classification_report(Y_test, predicted))\n",
    "print(metrics.confusion_matrix(Y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2.]\n"
     ]
    }
   ],
   "source": [
    "# once we have created the models, we can try to predict new cases using the models created\n",
    "newCases = [[14,2,2,15,110,2,2,0.5,1.5,4.8,1.2,4,1015],[13,1,1.5,14,120,3,1,0.3,1.3,4.1,1.7,5,1030]]\n",
    "predictedNewCases = model.predict(newCases)\n",
    "print(predictedNewCases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      0.50      0.67         2\n",
      "         2.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.50      0.25      0.33         2\n",
      "weighted avg       1.00      0.50      0.67         2\n",
      "\n",
      "[[1 1]\n",
      " [0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "newCases = [[14,2,2,15,110,2,2,0.5,1.5,4.8,1.2,4,1015],[13,1,1.5,14,120,3,1,0.3,1.3,4.1,1.7,5,1030]]\n",
    "Y_newCases = [1,1]\n",
    "predictedNewCases = model.predict(newCases)\n",
    "print(metrics.classification_report(Y_newCases, predictedNewCases))\n",
    "print(metrics.confusion_matrix(Y_newCases, predictedNewCases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=4, random_state=None, shuffle=False)\n",
      "TRAIN: [2 3 4 5 6 7] TEST: [0 1]\n",
      "TRAIN: [0 1 4 5 6 7] TEST: [2 3]\n",
      "TRAIN: [0 1 2 3 6 7] TEST: [4 5]\n",
      "TRAIN: [0 1 2 3 4 5] TEST: [6 7]\n"
     ]
    }
   ],
   "source": [
    "# The following is an example on how we can use KFold\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4],[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 1, 2, 1, 2, 1, 2])\n",
    "kf = KFold(n_splits=4)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)\n",
    "#KFold(n_splits=2, random_state=None, shuffle=False)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [1 2]\n",
      " [3 4]\n",
      " [1 2]\n",
      " [3 4]]\n",
      "[1 2 1 2 1 2]\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [1 2]\n",
      " [3 4]\n",
      " [1 2]\n",
      " [3 4]]\n",
      "[1 2 1 2 1 2]\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [1 2]\n",
      " [3 4]\n",
      " [1 2]\n",
      " [3 4]]\n",
      "[1 2 1 2 1 2]\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [1 2]\n",
      " [3 4]\n",
      " [1 2]\n",
      " [3 4]]\n",
      "[1 2 1 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    print(X_train)\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Naive Bayes\n",
      "**************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      0.98      0.99        59\n",
      "         2.0       0.99      0.99      0.99        71\n",
      "         3.0       0.98      1.00      0.99        48\n",
      "\n",
      "    accuracy                           0.99       178\n",
      "   macro avg       0.99      0.99      0.99       178\n",
      "weighted avg       0.99      0.99      0.99       178\n",
      "\n",
      "[[58  1  0]\n",
      " [ 0 70  1]\n",
      " [ 0  0 48]]\n"
     ]
    }
   ],
   "source": [
    "# 4. Segment the data in a training and test set with a random KFold split\n",
    "from sklearn import model_selection\n",
    "#X_train, X_test, Y_train, Y_test = model_selection.train_test_split(wineData, wineTarget, test_size = 0.30)\n",
    "kf = KFold(n_splits=4)\n",
    "kf.get_n_splits(wineData)\n",
    "print(\"\\n\\n Naive Bayes\")\n",
    "print(\"**************************************\")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "uniqueTargets = np.unique(wineTarget)\n",
    "for train_index, test_index in kf.split(wineData):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = wineData[train_index], wineData[test_index]\n",
    "    Y_train, Y_test = wineTarget[train_index], wineTarget[test_index]\n",
    "    model.partial_fit(X_train, Y_train, uniqueTargets)\n",
    "predicted = model.predict(wineData)\n",
    "print(metrics.classification_report(wineTarget, predicted))\n",
    "print(metrics.confusion_matrix(wineTarget, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
