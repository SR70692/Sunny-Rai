{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries (numpy, pandas, scikit-learn packages metrics and clustering)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Balance Scale dataset available at http://archive.ics.uci.edu/ml/datasets/balance+scale \n",
    "data = pd.read_csv(\"balance-scale.data\") #in the folder folder as the python .ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take a look at the dataset\n",
    "data #this is the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values #dataset converted to 2D-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 This data set was generated to model psychological experimental results. \n",
    "# Each example is classified as having the balance scale tip to the right, tip to the left, or be balanced. \n",
    "# The attributes are the left weight, the left distance, the right weight, and the right distance. \n",
    "# The correct way to find the class is the greater of (left-distance * left-weight) and (right-distance * right-weight). \n",
    "# If they are equal, it is balanced. \n",
    "# Segment the outcome (first column) and remaining data (attributes) so we can use the attributes for clustering\n",
    "X = data.values[:, 1:5] \n",
    "Y = data.values[:,0] #0 is for the index of the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(\"---------------------------------\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data that we are going to use for clustering\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example, not related to this dataset, but to better understand how scaling the dataset works\n",
    "arr1=np.array([[1,55,120],[4,2,334],[2,20,300],[1,26,923],[3,43,876],[1,53,55]])\n",
    "print(arr1, end=\"\\n------------------------\\n\")\n",
    "print(\"Before scaling, mean:\",arr1.mean(axis=0))\n",
    "print(\"Before scaling, std:\",arr1.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_arr1 = scale(arr1)\n",
    "print(scaled_arr1, end=\"\\n------------------------\\n\")\n",
    "print(\"After scaling, mean:\",scaled_arr1.mean(axis=0))\n",
    "print(\"After scaling, std:\",scaled_arr1.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before scaling, mean:\",arr1.mean(axis=0))\n",
    "print(\"After scaling, mean:\",scaled_arr1.mean(axis=0))\n",
    "print(\"Before scaling, std:\",arr1.std(axis=0))\n",
    "print(\"After scaling, std:\",scaled_arr1.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now back to our dataset\n",
    "# Scale the data that we are going to use for clustering\n",
    "scaled_data = scale(X)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data vs. scaled_data\n",
    "X = np.array(X,dtype=np.float64)\n",
    "print(\"scaled_data\")\n",
    "print(\"mean=\",scaled_data.mean(axis=0))\n",
    "print(\"std=\",scaled_data.std(axis=0))\n",
    "print(\"min=\",scaled_data.min(axis=0))\n",
    "print(\"max=\",scaled_data.max(axis=0))\n",
    "print(\"Not scaled_data\")\n",
    "print(\"mean=\",X.mean(axis=0))\n",
    "print(\"std=\",X.std(axis=0))\n",
    "print(\"min=\",X.min(axis=0))\n",
    "print(\"max=\",X.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that there are 3 possible categories for the data. \n",
    "# Create 3 data clusters using Agglomerative Hierarchical Clustering. \n",
    "# What are the silhouette score, homogeneity and completeness for these clusters? \n",
    "# (Helping hand, if you need to convert labels from strings to something else look at sklearn.preprocessing.LabelEncoder())\n",
    "\n",
    "#from sklearn import cluster\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "Y2 = LabelEncoder().fit_transform(Y)\n",
    "\n",
    "n_samples, n_features = scaled_data.shape\n",
    "n_digits = len(np.unique(Y))\n",
    "\n",
    "model = cluster.AgglomerativeClustering(n_clusters=n_digits, linkage=\"average\", affinity=\"cosine\")\n",
    "model.fit(scaled_data)\n",
    "\n",
    "#Silhouette refers to a method of interpretation and validation of consistency within clusters of data\n",
    "print(\"silhouette_score = \", metrics.silhouette_score(scaled_data, model.labels_))\n",
    "\n",
    "#all of the data points that we have of the same class are elements of the same cluster\n",
    "print(\"completeness_score = \", metrics.completeness_score(Y2, model.labels_))\n",
    "# all of the clusters contain only data points, which are members of a single class\n",
    "print(\"homogeneity_score = \", metrics.homogeneity_score(Y2, model.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)\n",
    "print(Y2)\n",
    "print(model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the impact of different distance and affinity measures on the silhouette score, homogeneity and \n",
    "# completeness for these clusters \n",
    "# (options available at http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)? \n",
    "# What is the best combination?\n",
    "\n",
    "#from sklearn import cluster\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "n_samples, n_features = scaled_data.shape\n",
    "n_digits = len(np.unique(Y))\n",
    "Y2 = LabelEncoder().fit_transform(Y)\n",
    "aff = [\"euclidean\", \"l1\", \"l2\", \"manhattan\", \"cosine\"]\n",
    "link = [\"ward\", \"complete\", \"average\"] \n",
    "for a in aff:\n",
    "    for l in link:\n",
    "        if(l==\"ward\" and a!=\"euclidean\"):\n",
    "           continue\n",
    "        else:\n",
    "            print(a,l)\n",
    "            model = cluster.AgglomerativeClustering(n_clusters=n_digits, linkage=l, affinity=a)\n",
    "            model.fit(scaled_data)\n",
    "            print(\"silhouette_score = \", metrics.silhouette_score(scaled_data, model.labels_))\n",
    "            print(\"completeness_score = \", metrics.completeness_score(Y2, model.labels_))\n",
    "            print(\"homogeneity_score = \", metrics.homogeneity_score(Y2, model.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What are the impact of different distance and affinity measures on the silhouette score, homogeneity and \n",
    "# completeness for these clusters \n",
    "# (options available at http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)? \n",
    "# What is the best combination?\n",
    "\n",
    "#from sklearn import cluster\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "n_samples, n_features = scaled_data.shape\n",
    "n_digits = len(np.unique(Y))\n",
    "Y2 = LabelEncoder().fit_transform(Y)\n",
    "aff = [\"euclidean\", \"l1\", \"l2\", \"manhattan\", \"cosine\"]\n",
    "link = [\"ward\", \"complete\", \"average\"] \n",
    "result = []\n",
    "for a in aff:\n",
    "    for l in link:\n",
    "        if(l==\"ward\" and a!=\"euclidean\"):\n",
    "           continue\n",
    "        else:\n",
    "            model = cluster.AgglomerativeClustering(n_clusters=n_digits, linkage=l, affinity=a)\n",
    "            model.fit(scaled_data)\n",
    "            result.append([a,l,metrics.silhouette_score(scaled_data, model.labels_),metrics.completeness_score(Y2, model.labels_),metrics.homogeneity_score(Y2, model.labels_)])\n",
    "maxI = -1\n",
    "maxV = 0\n",
    "for i in range(0,len(result)):\n",
    "  print(result[i])\n",
    "  if(result[i][2]>maxV):\n",
    "    maxV = result[i][2]\n",
    "    maxI = i\n",
    "print(\"Max silhouette_score: \", result[maxI])\n",
    "maxI = -1\n",
    "maxV = 0\n",
    "for i in range(0,len(result)):\n",
    "  #print(result[i])\n",
    "  if(result[i][3]>maxV):\n",
    "    maxV = result[i][3]\n",
    "    maxI = i\n",
    "print(\"Max completeness_score: \", result[maxI])\n",
    "maxI = -1\n",
    "maxV = 0\n",
    "for i in range(0,len(result)):\n",
    "  #print(result[i])\n",
    "  if(result[i][4]>maxV):\n",
    "    maxV = result[i][4]\n",
    "    maxI = i\n",
    "print(\"Max homogeneity_score: \", result[maxI])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What are the impact of different distance and affinity measures on the silhouette score, homogeneity and \n",
    "# completeness for these clusters \n",
    "# (options available at http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)? \n",
    "# What is the best combination?\n",
    "\n",
    "#from sklearn import cluster\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "n_samples, n_features = scaled_data.shape\n",
    "n_digits = len(np.unique(Y))\n",
    "Y2 = LabelEncoder().fit_transform(Y)\n",
    "aff = [\"euclidean\", \"l1\", \"l2\", \"manhattan\", \"cosine\"]\n",
    "link = [\"ward\", \"complete\", \"average\"]\n",
    "result = []\n",
    "for a in aff:\n",
    "    for l in link:\n",
    "      for i in range(2,10):\n",
    "        if(l==\"ward\" and a!=\"euclidean\"):\n",
    "           continue\n",
    "        else:\n",
    "            model = cluster.AgglomerativeClustering(n_clusters=i, linkage=l, affinity=a)\n",
    "            model.fit(scaled_data)\n",
    "            result.append([a,l,i,metrics.silhouette_score(scaled_data, model.labels_),metrics.completeness_score(Y2, model.labels_),metrics.homogeneity_score(Y2, model.labels_)])\n",
    "maxI = -1\n",
    "maxV = 0\n",
    "for i in range(0,len(result)):\n",
    "  #print(result[i])\n",
    "  if(result[i][3]>maxV):\n",
    "    maxV = result[i][3]\n",
    "    maxI = i\n",
    "print(\"Max silhouette_score: \", result[maxI])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the silhouette score, homogeneity and completeness for different numbers of clusters created using KMeans?\n",
    "#from sklearn import cluster\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "n_samples, n_features = scaled_data.shape\n",
    "n_digits = len(np.unique(Y))\n",
    "Y2 = LabelEncoder().fit_transform(Y)\n",
    "for k in range(2, 5):\n",
    "    kmeans = cluster.KMeans(n_clusters=k)\n",
    "    kmeans.fit(scaled_data)\n",
    "    print(k)\n",
    "    print(\"silhouette_score = \", metrics.silhouette_score(scaled_data, kmeans.labels_))\n",
    "    print(\"completeness_score = \", metrics.completeness_score(Y2, kmeans.labels_))\n",
    "    print(\"homogeneity_score = \", metrics.homogeneity_score(Y2, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dendrogram\n",
    "from scipy.cluster import hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "Z = hierarchy.linkage(X[0:20], 'complete')\n",
    "plt.figure(figsize=(10,5))\n",
    "dn = hierarchy.dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
